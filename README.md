Bastien E. - Mehdi B.  
25/01/2024


#  Projet Automatisation et Collecte des produits du site ACTION 

üï∏Ô∏è Webscraping Projet
Ce script Python r√©alise le webscraping d'informations produits √† partir du site web de l'entreprise ACTION  acc√®s depuis le site : https://www.action.com/. Explorez diff√©rentes cat√©gories, extrayez les donn√©es et enregistrez les informations individuelles de chaque produit en fichiers JSON.

# 1ere partie - Recup√©ration des donn√©es


## üõ†Ô∏è Pr√©requis

Assurons-nous d'avoir importer les biblioth√®ques suivantes avant d'ex√©cuter le script :


```python
import requests as rq
from bs4 import BeautifulSoup
import json
import os
import time 
```

- requests: Effectue des requ√™tes HTTP pour r√©cup√©rer des donn√©es depuis des URLs.
- BeautifulSoup: Facilite le web scraping en extrayant des informations √† partir de pages HTML ou XML.
- json: Manipule des donn√©es au format JSON.
- os: Fournit des fonctionnalit√©s pour interagir avec le syst√®me d'exploitation, notamment pour la manipulation de fichiers et de r√©pertoires.
- time: G√®re les aspects li√©s au temps, utilis√© ici pour introduire des pauses entre les requ√™tes HTTP.

## üß± Structure du Code
### Importations

Utilisation de requests pour effectuer des requ√™tes HTTP.
Utilisation de BeautifulSoup pour le web scraping.
Utilisation de json, os, et time.
Fonction get_token


```python
url_base = "https://www.action.com/fr-fr/"
user_agent = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"}
```

- Initialiser "url_base" avec l'URL de base du site web de l'entreprise ACTION
- Initialiser "user_agent" pour √©viter les blocages dus √† des requ√™tes non autoris√©es
et pour fournir des informations sur le client au serveur web.



D√©finition de l'URL de base et de l'en-t√™te utilisateur.
Boucle Principale sur les Cat√©gories et les Pages

Obtient le token n√©cessaire pour les requ√™tes.
Variables Initiales
```python
def get_token(url):
    reponse = rq.get(url, headers = user_agent)
    html_code = reponse.text
    soup = BeautifulSoup(html_code, 'html.parser')
    meta_tag = soup.find('meta', {'name': 'build-id'})
    build_id = meta_tag['content']
    return build_id
```

Le get_token prend une URL en param√®tre. Cette fonction envoie une requ√™te HTTP √† l'URL sp√©cifi√©e, r√©cup√®re le contenu HTML de la r√©ponse, puis utilise la biblioth√®que BeautifulSoup pour analyser le HTML. Et build-id pour d'autre requ√™tes.


```python
url_produit = []
dossier = "C:/Users/mbena/OneDrive/Bureau/python TP/Webscraping_Projet"
os.makedirs(dossier, exist_ok=True)

categories = ['habitat',
            'cuisine',
            'articles-menagers',
            'papeterie--bureau',
            'hobby',
            'bricolage',
            'jouets',
            'jardin',
            'voyages',
            'hygiene--beaute',
            'boissons--alimentation',
            'multimedia',
            'mode',
            'articles-de-sport',
            'animaux-domestiques']
```


Cr√©ation des cat√©gories pour le classement des articles depuis le site d'ACTION.

Dans notre projet, on se concentrera que sur la cat√©gorie "Habitat".


- Requ√™tes HTTP pour Obtenir les Informations Produits


- Parcourir les diff√©rentes cat√©gories de produits √† travers une boucle sur les pages de chaque cat√©gorie, effectue des requ√™tes HTTP pour chaque page, extrait les informations des produits, puis √©crit ces informations dans des fichiers JSON correspondants.

```python
for categorie in categories :
    print(f'categorie : {categorie}')
    for page in range(1, 2):
        #payload = {'locale': 'fr-fr', 'path': ['c', 'habitat'], 'page': page}
        payload = {'locale': 'fr-fr', 'path': ['c', categorie], 'page': page}
        #reponse = rq.get(f'https://www.action.com/_next/data/{get_token(url_base)}/fr-fr/c/habitat.json', params=payload, headers = user_agent)
        reponse = rq.get(f'https://www.action.com/_next/data/{get_token(url_base)}/fr-fr/c/{categorie}.json', params=payload, headers = user_agent)
        print(f'page {page}')
        page_reponse_dict = reponse.json()
        pageProps_dict = page_reponse_dict["pageProps"]
        try:
            initialState_dict = pageProps_dict["initialState"]    
        except : 
            break
        produit_list = [cle for cle in initialState_dict if cle.startswith("Product:")]
        info_produit_list = [initialState_dict.get(produit) for produit in produit_list]
        url_produit = [dict["href"] for dict in info_produit_list] 
        id_produit = [dict["id"] for dict in info_produit_list]
```


Enregistrement des informations des produits dans des fichiers JSON.
Pause en Cas de Redirection

```python
 for url_produit, id_produit in zip(url_produit, id_produit):
            code_name = url_produit.replace(url_base, "")
            code_name = code_name.replace("p/", "")
            code_name = code_name.split("/")
            if len(code_name) == 3:
                code_name.pop(2)
            reponse_produit = rq.get(f'https://www.action.com/_next/data/{get_token(url_base)}/fr-fr/p/{code_name[0]}/{code_name[1]}.json', params=payload, headers = user_agent)
            reponse_produit_json = reponse_produit.json()
            pageProps_dict2 = reponse_produit_json["pageProps"]

 if pageProps_dict2.get("__N_REDIRECT_STATUS") is not None :
                time.sleep(10)
                break

 initialState_dict2 = pageProps_dict2["initialState"] 
            product = initialState_dict2[f'Product:{id_produit}']                
            fichier_produit = os.path.join(dossier, f'{code_name[1]}.json')
            json_data = json.dumps(product)
            with open(fichier_produit, "w") as fichier_json:
                fichier_json.write(json_data)
```

Le code v√©rifie s'il y a une redirection avec la cl√© "__N_REDIRECT_STATUS". En cas de redirection, il fait une pause de 10 secondes et sort de la boucle. Si aucune redirection n'est d√©tect√©e, il extrait les informations du produit, les √©crit dans un fichier JSON et passe au produit suivant.
Le code, pour chaque article, compile ses informations (description, libell√©, id et prix) au format JSON dans le dossier d√©j√† cr√©e. Ces informations sont extraites, puis le code cr√©e un fichier JSON pour chaque article dans le dossier sp√©cifi√©.


#    2me partie - Gestion des donn√©es

## üõ†Ô∏è Pr√©requis

Pour le deuxi√®me jeu de donn√©es, on importe les biblioth√®ques suivantes avant d'ex√©cuter le script :


```python
import pandas as pd
import glob
import json
import matplotlib.pyplot as plt
```
- pandas: Manipule des donn√©es tabulaires, souvent utilis√© pour le traitement de donn√©es.
- glob: Recherche des noms de fichiers ou des chemins d'acc√®s en utilisant des motifs avec des caract√®res g√©n√©riques.
- json: Manipule des donn√©es au format JSON.
- matplotlib.pyplot: Cr√©e des visualisations, g√©n√©ralement des graphiques, √† partir des donn√©es.

## üìäüíª Structure du Code
### Importations

```python
def json_to_dataframe(file_path):
    with open(file_path, 'r') as f:
        data = json.load(f)
    df = pd.json_normalize(data)
    return df
```

- R√©cuperation des donn√©es des articles et de la gestion dans le dictionnaire df pour chaque article ensuite(id, code, titre, description, cat√©gorie, image, le prix etc.. )

- D√©finition du chemin vers un dossier contenant des fichiers JSON de produits, puis utilise la biblioth√®que glob pour cr√©er une liste de chemins vers tous les fichiers JSON dans ce dossier.

```python
dossier_produits = "C:/Users/mbena/OneDrive/Bureau/python TP/Webscraping_Projet"

fichiers_json = glob.glob(f"{dossier_produits}/*.json")
```
- Cr√©ation d'un DataFrame vide avec Pandas, puis it√®re sur une liste de chemins de fichiers JSON.√Ä chaque it√©ration, il convertit le contenu JSON de chaque fichier en DataFrame et concat√®ne ces DataFrames avec le DataFrame global combined_df. Enfin, il affiche les premi√®res lignes du DataFrame r√©sultant.

```python
combined_df = pd.DataFrame()

for fichier in fichiers_json:
    df = json_to_dataframe(fichier)
    combined_df = pd.concat([combined_df, df], ignore_index=True)

print(combined_df.head())
```

## üìà Graphiques

### Bo√Æte √† moustache pour price.current.amount et price.original.amount
```python
plt.figure(figsize=(10, 6))
combined_df.boxplot(column=['price.current.amount', 'price.original.amount'], vert=False)
plt.title('Comparaison des prix actuels et originaux pour les produits AUCHAN')
plt.xlabel('Prix (en unit√© de votre devise)')
plt.show()
```

### Histogramme pour price.original.amount
```python
plt.figure(figsize=(10, 6))
plt.hist(combined_df['price.current.amount'], bins=20, facecolor='lightblue', edgecolor='darkblue')
plt.title('Histogramme des prix des articles')
plt.xlabel('Prix (en euros)')
plt.ylabel('Fr√©quence')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()
```

### Histogramme pour price.current.amount
```python
plt.figure(figsize=(10, 6))
plt.hist(combined_df['price.original.amount'], bins=20, facecolor='lightcoral', edgecolor='darkblue')
plt.title('Histogramme des prix des articles en r√©duction')
plt.xlabel('Prix (en euros)')
plt.ylabel('Fr√©quence')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()
```
